## Blending Learning and Planning {#sec:blending-learning-planning-chapter}

After coverage of the issue of Exploration versus Exploitation in the last chapter, we dedicate this chapter to the topic of Planning versus Learning (and how to blend the two approaches) in the context of solving an MDP Control problem or solving an MRP Prediction problem. In this chapter, we also provide some coverage of the much-celebrated Monte-Carlo Tree-Search (abbreviated as MCTS) algorithm and it's spiritual origin - the Adaptive MultiStage Sampling (abbreviated as AMS) algorithm. MCTS and AMS are examples of how to blend Planning and Learning.

### Planning versus Learning

In the language of AI, we use the terms *Planning* and *Learning* to refer to two different approaches to solve an AI problem. Let us understand these terms from the perspective of solving Prediction (for an MRP) and Control (for an MDP). Let us zoom out and look at the big picture. We are given access to an MDP Environment $E$ (remember an MRP is simply an MDP evaluated with a fixed policy). The AI Agent (call it $A$) collects experiences data in the form of (next state, reward) for a current state and action in the process of *interacting* with the MDP Environment. The AI Agent's goal is to estimate the requisite Value Function/Policy through this process of interaction with the MDP Environment $E$ (for Prediction, the Agent estimates the Value Function for a given policy and for Control, the agent estimates the Optimal Value Function and the Optimal Policy). The AI Agent $A$ can go about this in one of two ways:

1. By interacting with the MDP Environment $E$, the AI Agent can build a model (call it $M$) and then use that model to estimate the requisite Value Function/Policy. We refer to this as the *Model-Based* approach. Solving Prediction/Control using a model is known as *Planning* the solution. The term *Planning* comes from the fact that the AI agent projects (with the help of the model $M$) probabilistic scenarios of future states/rewards for various choices of actions from specific states, and solves for the requisite Value Function based on the model-projected future outcomes. 
2. By interacting with the MDP Environment $E$, the AI Agent can directly estimate the requisite Value Function/Policy, without bothering to build a model. We refer to this as the *Model-Free* approach. Solving Prediction/Control without using a model is known as *Learning* the solution. The term *Learning* comes from the fact that the AI Agent "learns" the requisite Value Function/Policy directly from experiences data obtained by interacting with the MDP Environment $E$ (without requiring any model).

Let us now dive a bit deeper into both these approaches to understand them better.

#### Planning the solution of Prediction/Control

In the first approach (*Planning* the solution of Prediction/Control), we first need to "build a model". By "model", we refer to the State-Reward Transition Probability Function $\mathcal{P}_R$. By "building a model", we mean estimating the $\mathcal{P}_R$ from experiences data generated by interacting with the MDP Environment $E$. How does the AI Agent $A$ does this? Well, this is a matter of estimating the conditional probability density function of pairs of (next state, reward), conditioned on a particular pair of (state, action). This is an exercise in Supervised Learning, where the $y$-values are (next state, reward) pairs and the $x$-values are (state, action) pairs. We covered how to do Supervised Learning in Chapter [-@sec:funcapprox-chapter]. Also, note that Equation \eqref{eq:mrp-mle} in Chapter [-@sec:rl-prediction-chapter] shows the simple "tabular calculation" to estimate the $\mathcal{P}_R$ function for an MRP from a fixed, finite set of atomic experiences of (state, reward, next state) triples. We had written the function `finite_mrp` to construct a `FiniteMarkovRewardProcess` (which includes a tabular $\mathcal{P}_R$ function of explicit probabilities of transitions), given as input a `Sequence[TransitionStep[S]]` (i.e., fixed, finite set of atomic experiences that can be obtained by interacting with the MDP Environment $E$). Ok - now we have a model $M$ in the form of an estimated $\mathcal{P}_R$. The next thing is to do in this approach of *Planning* the solution of Prediction/Control is to use the model $M$ to estimate the requisite Value Function/Policy. There are two broad approaches to do this:

1. By constructing $\mathcal{P}_R$ as an explicit representation of probabilities of transitions, the AI Agent can utilize one of the Dynamic Programming Algorithms (eg: Policy Evaluation, Policy Iteration, Value Iteration) or a Tree-search method (by growing out a tree of future states/rewards/actions from a given state/sction, eg: the MCTS/AMS algorithms we will cover later in this chapter). Note that in this approach, there is *no need to interact with an MDP Environment* since a model of transition probabilities are available that can be used to project any (probabilistic) future outcome (for any choice of action) that is desired to estimate the requisite Value Function/Policy.
2. By treating $\mathcal{P}_R$ as a *sampling model*, by which we mean that the AI agent uses $\mathcal{P}_R$ as simply an (on-demand) interface to sample a pair of (next state, reward) from a given state and action. This means the AI Agent is treating this *sampling model* view of $\mathcal{P}_R$ as a *Simulated MDP Environment* (let us refer to this Simulated MDP Environment as $S$). Note that $S$ serves as a proxy/approximation of the actual MDP Environment $E$. A significant advantage of using $S$ instead of $E$ as the MDP Environment is that we can sample infinitely many times without any of the real-world interaction constraints that an actual MDP Environment $E$ poses. Think about a robot learning to walk on an actual street versus learning to walk on a simulator of the street's activities. Furthermore, the user could augment his/her views on top of an experiences-data-learnt simulator. For example, the user might say that the experiences data doesn't include certain types of scenarios but the user might have knowledge of how those scenarios would play out, thus creating a "human-knowledge-augmented simulator" (more on this in Chapter [-@sec:concluding-chapter]). By interacting with the simulated MDP Environment $S$ (instead of the actual MDP Environment $E$), the AI Agent can use any of the RL Algorithms we covered in Module III of this book to estimate the requisite Value Function/Policy. Since this approach uses a model $M$ (albeit a sampling model) and since this approach uses RL, we refer to this approach as *Model-Based RL*. To summarize this approach, we first learn (supervised learning) a model $M$ of the actual MDP Environment $E$ and then plan the solution to Prediction/Control by using the model $M$ in the form of a simulated MDP Environment $S$ which an RL algorithm interacts with. Here the Planning/Learning terminology often gets confusing to new students of this topic since this approach is supervised learning followed by planning (the planning being done with a Reinforcement Learning algorithm). Hopefully, the pictorial depiction of Figure \ref{model-based-rl} avoids the confusion.

#### Learning the solution of Prediction/Control

In the second approach (*Learning* the solution of Prediction/Control), we don't bother building a model. Rather, the AI Agent directly estimates the requisite Value Function/Policy from the experiences data generated by interacting with the Actual MDP Environment $E$. The AI Agent does this by using any of the RL algorithms we covered in Module III of this book. Since this approach is "model-free", we refer to this approach as *Model-Free RL*.

### Constructing a Model of the MDP Environment

A model $M$ of the environment has to be learnt as a probability-density estimation

### Blending Model-Free RL and Model-Based RL

### Monte-Carlo Tree Search

### Adaptive Multistage Sampling

### Summary of Key Learnings from this Chapter


